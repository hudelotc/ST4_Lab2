{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QICsOhm7j_uP"
   },
   "source": [
    "# Recherche d'Information et traitement de données massives\n",
    "## Lab 2 : Modèles de Recherche: Mise en oeuvre sur la collection TIME\n",
    "\n",
    "L'objectif de cette séance est de mettre en oeuvre les différents modèles de recherche vus en cours sur le corpus TIME, en utilisant la représentation de la collection sous la forme d'un index inversé et les différents algorithmes vus en cours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Charger l'index inversé de la collection TIME construit et sauvegardé sous la forme d'un fichier dans le LAB1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle booléen\n",
    "\n",
    "**Représentation des requêtes**\n",
    "\n",
    "Une des premières étapes pour la mise en oeuvre du modèle booléen est la représentation d'une requête. En effet, dans le modèle booléen, les requêtes sont des expressions booléennes pouvant être définies à l'aide des opérateurs logiques `OR`, `NOT` et `AND` comme par exemple la requête ` U.S AND War OR France`.  Nous avons vu dans le cours qu'il pouvait être intéressant de représenter les requêtes par leur [forme normale conjonctive](https://fr.wikipedia.org/wiki/Forme_normale_conjonctive) (conjonction de disjonctions) afin d'optimiser leur traitement à l'aide de l'index inversé. Dans la suite, nous supposerons donc que les requêtes sont exprimées souc cette forme. \n",
    "\n",
    "2- **Quelle structure de données informatique proposez-vous pour representer une requête sous forme normale conjonctive et faciliter son traitement ?** Puis, à partir de cette représentation, quelle stratégie mettre en place pour évaluer l'expression booléenne associée ?\n",
    "\n",
    "Indice :\n",
    "\n",
    "\n",
    "<img src=\"./Figures/booleantree.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "\n",
    "Une manière de faire pourrait être de récupérer les littéraux `U.S`et `France `et de leur appliquer l'opérateur `OR` puis de récupérer le littéral `War`de lui appliquer `NOT` et enfin d'appliquer l'opérateur `AND` aux deux résultats des évaluations précédentes. Si on écrit la formule selon cette stratégie, on obtient :\n",
    "` U.S France OR War NOT AND`. \n",
    "Cela correspond à la notation post-fixe de la formule. Cette notation, aussi connue sous le nom de [notation polonaise inversée](https://fr.wikipedia.org/wiki/Notation_polonaise_inverse) (en hommage à son créateur Jan Łukasiewicz grand logicien et philosophe polonais) est très pratique pour l'évaluation de la formule. Elle permet aussi de ne pas utiliser de parenthèses sans ambiguité.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer une expression en sa représentation sous forme d'arbre ou en sa notation post-fixe est un beau problème d'algorithmique et de programmation et vous pourrez y reflechir à la maison. Dans le Lab, pour nous éviter de passer trop de temps sur cette question, nous utiliserons la bibliothèque [`tt`](http://tt.brianwel.ch/en/latest/index.html) qui permet de travailler avec des expressions en logique booléenne sous python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installer la bibliothéque en executant la commande ci-dessous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ttable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous vous permet de transformer une requête exprimée sous la forme d'une chaîne de caractères en une formule booléeen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tt import BooleanExpression\n",
    "\n",
    "b = BooleanExpression('(War or France) and (not C)')\n",
    "print(b)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'instruction ci-dessous permet de récupérer l'arbre représentant la formule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.tree\n",
    "print(b.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est aussi possible de récupérer l'ensemble des tokens avec l'instruction ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " b.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même, il est possible de récupérer l'ensemble des tokens dans l'ordre post-fixe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.postfix_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la suite, on considérera donc deux cas:\n",
    "\n",
    "+ 1er cas : les requêtes sont données en langage naturel (comme par exemple dans le fichier [TIME.QUE](./Data/Time/TIME.QUE)), fournies avec la collection TIME et il faudra donc transformer cette requête en expression logique. En particulier, on considèrera que l'espace correspond à l'opérateur `AND`. Par exemple, la requête `KENNEDY ADMINISTRATION PRESSURE` correspond à la requête `kennedy AND administration AND pressure`. \n",
    "\n",
    "3- Ecrire une fonction `def transformation_query_to_boolean(query)` qui permet de transformer une requête en langage naturel sous sa forme logique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_query_to_boolean(query):\n",
    "    \n",
    "    # A completer\n",
    "    return boolean_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + 2ème cas : les requêtes sont fournies comme une expression logique (par exemple `'(War or France) and (not C)'`) et il s'agit de la transformer sous une forme post-fixe pour son evaluation.\n",
    "\n",
    "4- En utilisant la bibliothèque `tt` comme montré précedemment, ecrire une fonction permettant de transformer une requête en son ensemble de tokens ordonnés par ordre post-fixe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traitement des requêtes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit ici d'écrire une fonction `processing_boolean_query_with_inverted_index(booleanOperator,query, inverted_index)` qui permet de renvoyer la liste de documents pertinents pour la requête `query`. `booleanOperator`est l'ensemble des opérateurs booléens considérés et `inverted_index` est l'index inversé de la collection considérée.\n",
    "\n",
    "Quelques indications pour vous aider :\n",
    " + L'utilisation d'une structure de données [Pile](https://docs.python.org/3/tutorial/datastructures.html#using-lists-as-stacks) et de la notation polonaise inversée peut être utile pour l'évaluation de la requête complète.\n",
    " + Revoir le cours 2 sur le modèle booléen pour l'utilisation de l'index inversé dans ce cas.\n",
    " \n",
    "**On suppose que les listes de postings de l'index inversé sont triées par doc_id croissant**\n",
    "\n",
    "5- Pour faciliter l'écriture de cette fonction, nous allons d'abord écrire les fonctions :\n",
    "+ `def merge_and_postings_list(posting_term1,posting_term2)` qui applique l'opérateur AND à deux postings listes : intersection de posting liste (c.f. cours) ;\n",
    "+ `def merge_or_postings_list(posting_term1,posting_term2)` qui applique l'opérateur OR à deux postings listes : union de posting liste (c.f. cours) ;\n",
    "+ `def merge_and_not_postings_list(posting_term1,posting_term2)`qui calcule posting_term1 AND NOT posting_term2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_postings_list(posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    # A completer\n",
    "    return result\n",
    "\n",
    "# Test \n",
    "print(\"Postings du terme {} : {}\".format('NASSAU',document_index['NASSAU']))\n",
    "print(\"Postings du terme {} : {}\".format('FRANCE',document_index['FRANCE']))\n",
    "print(\"Test du AND\")\n",
    "res = merge_and_postings_list(document_index[\"NASSAU\"], document_index[\"FRANCE\"])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_or_postings_list(posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    # A completer\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test \n",
    "print(\"Postings du terme {} : {}\".format('NASSAU',document_index['NASSAU']))\n",
    "print(\"Postings du terme {} : {}\".format('FRANCE',document_index['FRANCE']))\n",
    "print(\"Test du OR\")\n",
    "res = merge_or_postings_list(document_index[\"NASSAU\"], document_index[\"FRANCE\"])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_not_postings_list(posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    # A completer\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test \n",
    "print(\"Postings du terme {} : {}\".format('NASSAU',document_index['NASSAU']))\n",
    "print(\"Postings du terme {} : {}\".format('FRANCE',document_index['FRANCE']))\n",
    "print(\"Test du AND_NOT\")\n",
    "res = merge_and_not_postings_list(document_index[\"NASSAU\"], document_index[\"FRANCE\"])\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- En utilisant les fonctions implémentées dans la question précédente, écrire la fonction `boolean_operator_processing_with_inverted_index(BoolOperator,posting_term1,posting_term2)` qui applique l'opérateur `BoolOperator` parmi (AND, OR et AND NOT) à deux posting listes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_operator_processing_with_inverted_index(BoolOperator,posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    # A completer\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- Ecrire la fonction `def processing_boolean_query_with_inverted_index(booleanOperator,query, inverted_index)` qui permet de renvoyer la liste de documents pertinents pour la requête `query`. `booleanOperator`est l'ensemble des opérateurs booléens considérés et `inverted_index` est l'index inversé de la collection considérée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_boolean_query_with_inverted_index(booleanOperator,query, inverted_index):\n",
    "    # A completer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle vectoriel\n",
    "\n",
    "**Représentation des requêtes et des documents**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le modèle vectoriel, les requêtes et les documents ont le même type de représentation, un vecteur pondéré dans l'espace des termes d'indexation. Il est donc nécessaire de faire subir à votre requête, souvent exprimée en langage naturel, les mêmes traitements que ceux appliqués aux documents lors de la phase d'indexation  soit :\n",
    "+ L'étape de segmentation ;\n",
    "+ L'étape de filtrage ;\n",
    "+ L'étape de normalisation.\n",
    "\n",
    "Cela permettra notamment de garantir que l'espace de représentation de la requête et du document sont les mêmes.\n",
    "\n",
    "8- Ecrire une fonction `pre_processed_query(query)`qui prend en entrée une requête sous la forme d'une chaîne de caractères et qui renvoie la requête sous la forme d'une liste de termes filtrés et normalisés. On pourra pour cela ré-utiliser les fonctions écrites dans le Lab 1 et fournies dans le fichier [Lab1.py](./Utils/Lab1.py) du répertoire [Utils](./Utils).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Lab1 import *\n",
    "\n",
    "def pre_processed_query(query):\n",
    "    # A completer\n",
    "    return []\n",
    "\n",
    "    \n",
    "# Test\n",
    "pre_processed_query(\"KENNEDY ADMINISTRATION PRESSURE ON NGO DINH DIEM TO STOP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un choix doit cependant être fait sur la manière de pondérer les différents termes de la requête et des documents lors de la construction de leurs représentations. En particulier, différents schémas de pondération sont possibles.\n",
    "Il peut être intéressant de pouvoir comparer ces différents types de pondération ainsi, dans la suite nous nous intéresserons aux schémas de pondérations suivants :\n",
    "+ **Pondération binaire** comme dans le modèle booléen ;\n",
    "+ **Pondération `tf`** avec  $w_{id} = tf_{t_i,d}$ où $d$ peut être un document ou une requête ;\n",
    "+ **Pondération `tf-idf`** avec donc $w_{id} = tf_{t_i,d} \\times idf_{t_i}$ où $idf_{t_i} = \\log{\\frac{N}{df_{t_i}}}$ et $N$ étant le nombre de documents dans la collection ;\n",
    "+ **Pondération fréquentielle normalisée** qui consiste à normaliser les fréquences des termes apparaissant dans un document $d$ par la fréquence maximale dans le document $tf_{max_d} = \\max_{t \\in d} tf_{t,d}$ soit pour le terme $p_{tf_{t_i,d}}$ correspondant à la fréquence dans la collection la formule :\n",
    "$ p_{tf_{t_i,d}} = 0.5 + 0.5 \\times \\frac{tf_{t_i,d}}{tf_{max_d}}$ ;\n",
    "+ **Pondération logarithmique des termes** avec $p_{tf_{t_i,d}} = 1 + \\log{tf_{t_i,d}}$ si $tf_{t_i,d} > 0$ et $0$ sinon ;\n",
    "+ **Pondération logarithmique des termes normalisée** qui consiste à normaliser la caractéristique logarithmique précédente par une quantité dépendant de la moyenne $moy_{tf_d} = \\sum_{i=1}^{V}\\frac{tf_{t_i,d}}{|d|}$ ($|d|$ est le nombre de termes uniques du document $d$) des caractéristiques fréquentielles d'un document $d$ soit $ p_{tf_{t_i,d}} = \\frac{1 + \\log{tf_{t_i,d}}}{1 + \\log{moy_{tf_d}}}$ ;\n",
    "+ Et bien entendu, pour chacune de ces différentes pondérations fréquentielles des termes, il est possible de multiplier ou non par le poids du terme dans le collection $p_{df_{t_i}}$ qui peut être soit $idf_{t_i}$ soit une version normalisée $ max(0, \\log{\\frac{N -df_{t_i}}{df_{t_i}}})$.\n",
    "\n",
    "Ces schémas de pondération peuvent être appliqués au document et à la requête. Votre premier travail consiste donc à écrire les fonctions permettant de calculer à partir de l'index inversé et d'informations statistiques additionnelles sur la collection ces différents schémas de pondération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- Ecrire une fonction `get_tf (term,doc_ID, index_frequence)` qui permet de récupérer la fréquence d'un terme dans un document à partir d'un index de fréquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.saveandload_pickle import *\n",
    "\n",
    "# Chargement de l'index de frequence \n",
    "frequence_index = load_inverted_index_pickle('./Index/index_frequence.pickle')\n",
    "\n",
    "def get_tf(term,doc_ID,index_frequence):\n",
    "    # A completer\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- Ecrire une fonction `get_tf_logarithmique (term,doc_ID, index_frequence)` qui permet de calculer la pondération logarithmique d'un terme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_logarithmique (term,doc_ID, index_frequence):\n",
    "    # A completer\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les autres schémas de pondération, il est nécessaire d'avoir au préalable calculé et stocké plusieurs informations statistiques sur la collection comme le nombre total de documents et pour chaque document, la fréquence maximale dans le document, le nombre de termes uniques dans le document ou encore la moyenne des fréquences dans un document. \n",
    "\n",
    "11- Ecrire une fonction `get_stats_document(document)` qui pour un document donné sous la forme d'une liste de tokens renvoie un dictionnaire avec les informations statistiques mentionnées ci-dessus (clés : \"freq_max\", \"unique_terms\", \"freq_moy\"). On pourra utiliser la collection `Counter` du module `collections` de la bibliothèque standard de python comme dans le LAB1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_stats_document(document):\n",
    "    stats={}\n",
    "    # A completer\n",
    "    return stats\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12- Ecrire une fonction `get_stats_collection(collection)` qui permet de calculer des informations statistiques sur la collection comme le nombre de documents et pour chaque document de la collection, les informations statistiques précédentes.\n",
    "La fonction retournera un dictionnaire avec les clés \"nb_docs\" et les doc_ID des documents de la collection. On supposera que la collection passée en paramètre est une collection segmentée et pré-traitée (filtrage et normalisation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_collection(collection):\n",
    "    stats={}\n",
    "    # A completer\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13- Appliquer cette fonction à la collection TIME. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Lab1 import *\n",
    "\n",
    "collection_TIME = loadData(\"./Data/Time/TIME.ALL\")\n",
    "\n",
    "pre_processed_collection_TIME = collection_lemmatize(remove_stop_words(tokenize_Regexp_corpus(collection_TIME),\"./Data/Time/TIME.STP\"))\n",
    "\n",
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14- Ecrire une fonction `get_tf_normalise(term,doc_ID, index_frequence,stats_collection)` qui calcule la pondération fréquentielle du terme `term` pour le document d'identifiant `doc_ID` à partir de l'index de fréquence `index_frequence` et d'informations statistiques sur la collection stockée dans le dictionnaire `stats_collection` construit à l'étape précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_normalise(term,doc_ID, index_frequence,stats_collection):\n",
    "        # A completer\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15- Ecrire une fonction `get_tf_logarithme_normalise(term,doc_ID, index_frequence,stats_collection)` qui calcule la pondération fréquentielle logarithmique normalisée du terme `term` pour le document d'identifiant `doc_ID` à partir de l'index de fréquence `index_frequence` et d'informations statistiques sur la collection stockées dans le dictionnaire `stats_collection` construit à l'étape précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_logarithme_normalise(term,doc_ID, index_frequence,stats_collection):\n",
    "        # A completer\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16- Ecrire une fonction `get_idf (term ,index_frequence, nb_doc)` qui permet de calculer le logarithme de l'inverse normalisée de la fréquence documentaire $df_t$ d'un terme. `nb_doc`est le nombre de documents dans la collection. Il est égal à 523 pour la collection TIME ou vous pouvez le récupérer via le dictionnaire de statistiques construit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "\n",
    "def get_idf(term,index_frequence,nb_doc):\n",
    "    # A completer\n",
    "    return \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traitement des requêtes**\n",
    "\n",
    "17- Ecrire une fonction `processing_vectorial_query(query, inverted_index,stats_collection, weighting_scheme_document,weighting_scheme_query)` qui permet de renvoyer la liste de documents pertinents, ordonnés par pertinence, pour la requête `query` à partir de l'index de fréquence `inverted_index`. Les paramètres `weighting_scheme_document` et `weighting_scheme_query` permettent de définir le type de pondération choisie pour le document et pour la requête. On considérera les types suivants :\n",
    " + 'binary' : schema de pondération binaire.\n",
    " + 'frequency' : schema de pondération fréquentiel simple (`tf` seul).\n",
    " + 'tf_idf_normalize' : schema de pondération `tf_idf` avec la pondération fréquentielle normalisée pour le terme correspondant à la fréquence du terme dans le document.\n",
    " + 'tf_idf_logarithmic' : schema de pondération `tf_idf` avec la pondération fréquentielle logarithmique pour le terme correspondant à la fréquence du terme dans le document.\n",
    " + 'tf_idf_logarithmic_normalize' : schema de pondération `tf_idf` avec la pondération fréquentielle logarithmique normalisée pour le terme correspondant à la fréquence du terme dans le document.\n",
    "\n",
    "\n",
    "Vous pourrez pour cela vous appuyer sur l'algorithme décrit dans le support du cours 1 (version longue), slide 136. On ne considérera pour la requête que les schémas de pondération ('binary','frequency') et on prendra comme mesure de similarité le produit scalaire. On ne considérera donc pas de facteur de normalisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_vectorial_query(query, inverted_index, stats_collection, weighting_scheme_document,weighting_scheme_query):\n",
    "    ordered_relevant_docs = {}\n",
    "    # a completer\n",
    "    return ordered_relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18- Tester votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab2_ModelesDeRecherche.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
